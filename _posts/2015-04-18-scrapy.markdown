---
layout: post
title:  "python爬虫训练-Scrapy"
date:   2015-04-18 16:58:57
published: true
categories: opensource
---

* 目录
{:toc}

## 打算插一段centos 升级python 的坑

今天下午（2015-6-19）尝试在阿里云服务器安装scrapy 时 需要升级python 到2.7 遇到了不少坑

把 centos 升级python 的方法 在这里赘述下

{% highlight sh %}
# 更新工具
yum -y update
yum groupinstall -y 'development tools'

# 安装必要的包

yum install -y zlib-devel bzip2-devel openssl-devel xz-libs wget
yum install libffi-devel # 这个曾经漏掉 导致 scrapy 安装失败

# 源码编译安装python  --prefix 默认:/usr/local
# 不赘述了

# 更新到环境变量
export PATH="/usr/local/bin:$PATH"
# or 
ln -s /usr/local/bin/python2.7  /usr/bin/python # 这种方式需要修复yum

# setuptools

# 获取软件包
wget --no-check-certificate https://pypi.python.org/packages/source/s/setuptools/setuptools-1.4.2.tar.gz
# 解压:
tar -xvf setuptools-1.4.2.tar.gz
cd setuptools-1.4.2
# 使用 Python 2.7.8 安装 setuptools
python2.7 setup.py install

# pip

curl https://raw.githubusercontent.com/pypa/pip/master/contrib/get-pip.py | python2.7 -

# 修复yum

[root@iZ25ifl9i5nZ ~]# which yum 
/usr/bin/yum
#修改 yum中的python 
#将第一行  #!/usr/bin/python  改为 #!/usr/bin/python2.6
#此时yum就ok啦

{% endhighlight %}


##简介

### 创建项目

{% highlight sh %}

scrapy startproject tutorial

{% endhighlight %}

项目目录 ./tutorial

{% highlight sh %}

|____scrapy.cfg  # 项目配置文件
|____tutorial  # 项目python模块目录
| |______init__.py 
| |____items.py  # 项目items文件
| |____pipelines.py # 项目管道文件
| |____settings.py # 项目配置文件
| |____spiders # 放置spider文件的目录
| | |______init__.py

{% endhighlight %}

### 第一个例子

items.py

{% highlight python %}

# -*- coding: utf-8 -*-

# Define here the models for your scraped items
#
# See documentation in:
# http://doc.scrapy.org/en/latest/topics/items.html

import scrapy

# 系统自动构建的item
class TutorialItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    pass


#第一个自定义的item 
class DmozItem(scrapy.Item):
    title = scrapy.Field()
    link = scrapy.Field()
    desc = scrapy.Field()

{% endhighlight %}

定义spider
spiders/dmoz_spider.py

{% highlight python %}

__author__ = 'zhangshuang'

import scrapy


class DmozSpider(scrapy.Spider):
    name = "dmoz"
    allowed_domains = ["dmoz.org"]
    start_urls = [
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"
    ]

    def parse(self, response):
        filename = response.url.split("/")[-2]
        with open(filename, 'wb') as f:
            f.write(response.body)
            
{% endhighlight %}

tutorial目录下执行

{% highlight sh %}

scrapy crawl dmoz

{% endhighlight %}

commandline tool输出

{% highlight sh %}

2015-04-22 10:42:55+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: tutorial)
2015-04-22 10:42:55+0800 [scrapy] INFO: Optional features available: ssl, http11, django
2015-04-22 10:42:55+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'tutorial.spiders', 'SPIDER_MODULES': ['tutorial.spiders'], 'BOT_NAME': 'tutorial'}
2015-04-22 10:42:55+0800 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-22 10:42:55+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-22 10:42:55+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-22 10:42:55+0800 [scrapy] INFO: Enabled item pipelines: 
2015-04-22 10:42:55+0800 [dmoz] INFO: Spider opened
2015-04-22 10:42:55+0800 [dmoz] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-04-22 10:42:55+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2015-04-22 10:42:55+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6080
2015-04-22 10:42:56+0800 [dmoz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/> (referer: None)
2015-04-22 10:42:56+0800 [dmoz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)
2015-04-22 10:42:56+0800 [dmoz] INFO: Closing spider (finished)
2015-04-22 10:42:56+0800 [dmoz] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 516,
	 'downloader/request_count': 2,
	 'downloader/request_method_count/GET': 2,
	 'downloader/response_bytes': 16374,
	 'downloader/response_count': 2,
	 'downloader/response_status_count/200': 2,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 4, 22, 2, 42, 56, 466282),
	 'log_count/DEBUG': 4,
	 'log_count/INFO': 7,
	 'response_received_count': 2,
	 'scheduler/dequeued': 2,
	 'scheduler/dequeued/memory': 2,
	 'scheduler/enqueued': 2,
	 'scheduler/enqueued/memory': 2,
	 'start_time': datetime.datetime(2015, 4, 22, 2, 42, 55, 798784)}
2015-04-22 10:42:56+0800 [dmoz] INFO: Spider closed (finished)

{% endhighlight %}

dmoz 日志可以看到访问了 start_urls 中的两个url并且 在  ./ 目录下生成了 Book 和Resources 两个文件
该程序就是将两个页面的源码分别保存到Book和Resources下。

### command line tool 下调试selector

selector 提供的方法：

* xpath(): returns a list of selectors, each of them representing the nodes selected by the xpath expression given as argument.
* css(): returns a list of selectors, each of them representing the nodes selected by the CSS expression given as argument.
* extract(): returns a unicode string with the selected data.
* re(): returns a list of unicode strings extracted by applying the regular expression given as argument.

command line tool 下执行

{% highlight sh %}

scrapy shell "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/"

{% endhighlight %}

log

{% highlight sh %}

2015-04-22 11:04:36+0800 [scrapy] INFO: Scrapy 0.24.4 started (bot: tutorial)
2015-04-22 11:04:36+0800 [scrapy] INFO: Optional features available: ssl, http11, django
2015-04-22 11:04:36+0800 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'tutorial.spiders', 'SPIDER_MODULES': ['tutorial.spiders'], 'LOGSTATS_INTERVAL': 0, 'BOT_NAME': 'tutorial'}
2015-04-22 11:04:36+0800 [scrapy] INFO: Enabled extensions: TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-04-22 11:04:36+0800 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-04-22 11:04:36+0800 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-04-22 11:04:36+0800 [scrapy] INFO: Enabled item pipelines: 
2015-04-22 11:04:36+0800 [scrapy] DEBUG: Telnet console listening on 127.0.0.1:6023
2015-04-22 11:04:36+0800 [scrapy] DEBUG: Web service listening on 127.0.0.1:6080
2015-04-22 11:04:36+0800 [dmoz] INFO: Spider opened
2015-04-22 11:04:37+0800 [dmoz] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)
[s] Available Scrapy objects:
[s]   crawler    <scrapy.crawler.Crawler object at 0x105c0d2d0>
[s]   item       {}
[s]   request    <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>
[s]   response   <200 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/>
[s]   settings   <scrapy.settings.Settings object at 0x105020190>
[s]   spider     <DmozSpider 'dmoz' at 0x106911810>
[s] Useful shortcuts:
[s]   shelp()           Shell help (print this help)
[s]   fetch(req_or_url) Fetch request (or URL) and update local objects
[s]   view(response)    View response in a browser

{% endhighlight %}

打开 http://www.dmoz.org/Computers/Programming/Languages/Python/Books/ 的html document调试

 一些调用的例子
 
{% highlight sh %}

In [1]: response.xpath(’//title’)
Out[1]: [<Selector xpath=’//title’ data=u’<title>Open Directory - Computers: Progr’>]
In [2]: response.xpath(’//title’).extract()
Out[2]: [u’<title>Open Directory - Computers: Programming: Languages: Python: Books</title>’]
In [3]: response.xpath(’//title/text()’)
Out[3]: [<Selector xpath=’//title/text()’ data=u’Open Directory - Computers: Programming:’>]
In [4]: response.xpath(’//title/text()’).extract()
Out[4]: [u’Open Directory - Computers: Programming: Languages: Python: Books’]
In [5]: response.xpath(’//title/text()’).re(’(\w+):’)

Out[5]: [u’Computers’, u’Programming’, u’Languages’, u’Python’]

{% endhighlight%}

selector parse 的一个例子

spiders/dmoz_spider.py

{% highlight python %}

__author__ = 'zhangshuang'

import scrapy


class DmozSpider(scrapy.Spider):
    name = "dmoz"
    allowed_domains = ["dmoz.org"]
    start_urls = [
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"
    ]

    def parse(self, response):
        for sel in response.xpath('//ul/li'):
            title = sel.xpath('a/text()').extract()
            link = sel.xpath('a/@href').extract()
            desc = sel.xpath('text()').extract()
            print title, link, desc


{% endhighlight%}

该示例会将start_urls 中页面的document 的 ul/li 下a的text(赋值title) 和 href(赋值link) 及 ul/li 的 text(赋值desc) 打印

### 自定义的item

spiders/dmoz_spider.py

{% highlight python %}

__author__ = 'zhangshuang'

import scrapy

from tutorial.items import DmozItem

class DmozSpider(scrapy.Spider):
    name = "dmoz"
    allowed_domains = ["dmoz.org"]
    start_urls = [
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
        "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/"
    ]

    def parse(self, response):
        for sel in response.xpath('//ul/li'):
            item = DmozItem()
            item['title'] = sel.xpath('a/text()').extract()
            item['link'] = sel.xpath('a/@href').extract()
            item['desc'] = sel.xpath('text()').extract()
            yield item

{% endhighlight %}

针对上面的spider 执行时添加选项

{% highlight python %}

scrapy crawl dmoz -o items.json

{% endhighlight%}

会将结果以json 的格式存储到items.json文件中
 
## 基础概念

### scrapy支持的命令

* 全局命令

startproject

{% highlight sh %}

scrapy startproject <project_name>

{% endhighlight %}

create a new scrapy project.


settings

runspider

shell

fetch

view

version

* 项目内部命令

crawl

check

list

edit

parse

genspider

deploy

bench

### Items

定义一个item

{% highlight python %}

class Product(scrapy.Item):
    name = scrapy.Field()
    price = scrapy.Field()
    stock = scrapy.Field()
    last_update = scrapy.Field()
    
{% endhighlight %}

item的创建

* 从item中获取某个属性的值

* 设置item中某个属性的值

* 获取item中的所有的属性和属性的值

* 复制item

* item和dict的值换换

* item可以被继承并且在子类中增加一些属性


### Spiders

Spider

* name

spider 的id，标识一个spider，要保证唯一性

* allowed_domains

允许spider crawl的domain，在设置了OffsiteMiddleware的情况下不在allowed_domains下的start_url 不会被crawl

* start_urls

spider 从该属性提供的url列表开始crawl

* start_requests()

将urls生成为Requests，现在针对start_urls指定的url会调用make_reqeusts_from_url()方法获取Requests,而其他没有被start_urls指定的url可以通过该方法生成Request


{% highlight python %}

def start_requests(self):
    return [scrapy.FormRequest("http://www.example.com/login", formdata={'user': 'john', 'pass': 'secret'}, callback=self.logged_in)]

def logged_in(self, response):
    pass    

{% endhighlight%}

* make_requests_from_url(url)

将start_url创建为Request并将parse() 作为callback

* parse(response)

scrapy拿到response后默认的callback函数，返回一个Request的迭代器或者是Item

* log(message [,level,component])

日志记录方法

* close()

spider关闭时会执行的方法

一些spider的简单示例


{% highlight python %}

__author__ = 'zhangshuang'

import scrapy

class MySpider(scrapy.Spider):
    name = 'example.com'
    allowed_domains = ['example.com']
    start_urls = [
        'http://www.example.com/1.html',
        'http://www.example.com/2.html',
        'http://www.example.com/3.html',
    ]

    def parse(self, response):
        self.log('A message from %s just arrived ' % response.url)
        for h3 in response.xpath('//h3').extract():
            yield MyItem(title=h3)

        for url in response.xpath('//a/@href').extract():
            yield scrapy.Request(url. callback=self.parse)

{% endhighlight %}


### Selectors

selector 可以从document 中选取到特定标签下的元素
一个selector使用的示例

item

{% highlight python %}

class ApkItem(scrapy.Item):
    apk_name = scrapy.Field()
    apk_pic = scrapy.Field()
    apk_desc = scrapy.Field()
    apk_down_url = scrapy.Field()
    apk_score = scrapy.Field()
    apk_info_url = scrapy.Field()

{% endhighlight%}

spider

{% highlight python %}

class ApkSpider (scrapy.Spider):
    name = "apk"
    allowed_domains = ["hiapk.com"]
    start_urls = [
        "http://www.example.com/xxx.html"
    ]

    def parse(self, response):
        # 通过这些selector 提供的如下方法可以找到具体某一个标签下的元素
        for sel in response.xpath('//ul/li[contains(@class, "list_item")]'):
            item = ApkItem()
            item['apk_name'] = sel.xpath('div/dl/dt/span[contains(@class , "list_title")]/a/text()').extract()
            item['apk_info_url'] = sel.xpath('div/div[contains(@class, "left")]/a/@href').extract()
            item['apk_down_url'] = sel.xpath('div/div[contains(@class, "button_bg")]/a/@href').extract()
            # item['apk_down_url'] = sel.css('div div[class*=button_bg] a::attr(href)').extract() # css 方式获取特定标签
            item['apk_desc'] = sel.xpath('div/dl/dd/div[contains(@class, "list_description")]/text()').extract()
            yield item

{% endhighlight %}

除了xpath的方法外 还有css 方法 也可以起到相同的作用，相关用法可以参考官方的文档。

Selector 提供的方法

* xpath()

* css()

* extract()

* re() 
该selector 匹配一个正则表达式的所有元素的列表

* register_namespace(prefix, uri)

* remove_namespace()

* \__nonzero__()

### Item Loaders

